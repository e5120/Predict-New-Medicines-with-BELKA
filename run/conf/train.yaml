hydra:
  job:
    name: train
    chdir: true
  run:
    dir: ${dir.model_dir}/${exp_name}/${dir_name}
  sweep:
    dir: ${dir.model_dir}/${exp_name}
    subdir: run${hydra.job.num}

defaults:
  - _self_
  - dir: local
  - dataset: lm_dataset
  - model: lm_model
  - optimizer: radam
  - scheduler: constant

data_dir: /project/data/lb
dir_name: single
exp_name: dummy
stage: train
batch_size: 256
seed: 42
num_workers: 0
benchmark: False
logger: False

trainer:
  max_epochs: 10
  min_epochs: 5
  enable_progress_bar: True
  accelerator: auto
  precision: "16-mixed"
  gradient_clip_val: ~
  accumulate_grad_batches: 1
  # reload_dataloaders_every_n_epochs: 1
  devices: [0]

# early stopping
early_stopping:
  monitor: "val_map"
  mode: "max"
  patience: 3

# model checkpoint
model_checkpoint:
  save_weights_only: True
  monitor: "val_map"
  mode: "max"
  dirpath: True
  save_top_k: 1
  verbose: 1

# train-val split
n_rows: 1000000
bb1_frac: 0.2
bb2_frac: 0.4
bb3_frac: 0.4
